{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cc67735f-9fad-4a8f-bdd5-000e2e8bc79d",
   "metadata": {},
   "source": [
    "# **Introduction to LangChain**\n",
    "\n",
    "**Build Applications That Can Reason**\n",
    "\n",
    "LangChain is a framework for developing applications powered by language models.  \n",
    "LangChain enables building application that connect external sources of data and computation to LLMs. \n",
    "\n",
    "It enables applications that:\n",
    "\n",
    "- **Are context-aware:** connect a language model to sources of context (prompt instructions, few shot examples, content to ground its response in, etc.)\n",
    "- **Reason:** rely on a language model to reason (about how to answer based on provided context, what actions to take, etc.)\n",
    "\n",
    "This framework consists of several parts.\n",
    "\n",
    "- **LangChain Libraries:** The Python and JavaScript libraries. Contains interfaces and integrations for a myriad of components, a basic run time for combining these components into chains and agents, and off-the-shelf implementations of chains and agents.\n",
    "- **LangChain Templates:** A collection of easily deployable reference architectures for a wide variety of tasks.\n",
    "- **LangServe:** A library for deploying LangChain chains as a REST API.\n",
    "- **LangSmith:** A developer platform that lets you debug, test, evaluate, and monitor chains built on any LLM framework and seamlessly integrates with LangChain.\n",
    "\n",
    "Together, these products simplify the entire application lifecycle:\n",
    "\n",
    "- **Develop:** Write your applications in LangChain/LangChain.js. Hit the ground running using Templates for reference.\n",
    "- **Productionize:** Use LangSmith to inspect, test and monitor your chains, so that you can constantly improve and deploy with confidence.\n",
    "- **Deploy:** Turn any chain into an API with LangServe.\n",
    "\n",
    "### **Langchain vs Langchain-Community vs Langchain-Core**  \n",
    "The old langchain package into three separate packages to improve developer experience\n",
    "\n",
    "`langchain-core` contains simple, core abstractions that have emerged as a standard, as well as LangChain Expression Language as a way to compose these components together. This package is now at version 0.1 and all breaking changes will be accompanied by a minor version bump.\n",
    "\n",
    "`langchain-community` contains all third party integrations. We will work with partners on splitting key integrations out into standalone packages over the next month.\n",
    "\n",
    "`langchain` contains higher-level and use-case specific chains, agents, and retrieval algorithms that are at the core of your application's cognitive architecture. We are targeting a launch of a stable 0.1 release for langchain in early January. The latest version (v2) came in March2024\n",
    "\n",
    "<img src=\"images/langchain_stack.JPG\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f52685f3-d9f4-44b8-8e6a-364dd6c4c7fa",
   "metadata": {},
   "source": [
    "## **Prompt Template + Model + Chains + Output Parser**\n",
    "\n",
    "**Model**  \n",
    "- LLMs handle various language operations such as translation, summarization, question answering, and content creation.\n",
    "\n",
    "**Prompt Template**  \n",
    "- Prompt Templates are used to convert raw user input to a better input to the LLM. Templates allow us to easily configure and modify our input prompts to LLM calls.\n",
    "\n",
    "**Output Parser**  \n",
    "- It's often much more convenient to work with strings. We can add a simple output parser to convert the chat message to a string.\n",
    "\n",
    "**Chains**  \n",
    "- The `|` symbol chains together the different components feeds the output from one component as input into the next component.\n",
    "\n",
    "\n",
    "<img src=\"images/langchain_LCEL.JPG\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0882d9fe-3ee2-4be2-a2d4-b2237ea249c1",
   "metadata": {},
   "source": [
    "# Step 1 - Models (LLM and ChatLLM)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ba690292-cd5c-4503-a79d-9ab25d94db78",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup API key\n",
    "f = open('keys/.openai_api_key.txt')\n",
    "OPEN_API_KEY = f.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e47f1a83-9931-4739-84a0-ea83a32ce98f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "content='Feature engineering is the process of selecting, creating, and transforming variables (features) in a dataset to improve the performance of machine learning algorithms. It involves identifying the most relevant features, handling missing data, encoding categorical variables, scaling numerical variables, and creating new features through techniques like polynomial features, interaction terms, and feature selection. Feature engineering is a crucial step in the data preprocessing pipeline as it can significantly impact the accuracy and efficiency of machine learning models.' additional_kwargs={'refusal': None} response_metadata={'token_usage': {'completion_tokens': 90, 'prompt_tokens': 15, 'total_tokens': 105, 'completion_tokens_details': {'audio_tokens': None, 'reasoning_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': None, 'cached_tokens': 0}}, 'model_name': 'gpt-3.5-turbo-0125', 'system_fingerprint': None, 'finish_reason': 'stop', 'logprobs': None} id='run-7007b117-a1ce-4e28-ac81-2eb79963455e-0' usage_metadata={'input_tokens': 15, 'output_tokens': 90, 'total_tokens': 105}\n"
     ]
    }
   ],
   "source": [
    "# Import OpenAI ChatModel\n",
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "# Set the ChatOpenAI KEY and initialise a model\n",
    "chat_model = ChatOpenAI(api_key=OPEN_API_KEY)\n",
    "\n",
    "# creating a promp\n",
    "prompt = \"What is Feature Engineering in Data Science?\"\n",
    "\n",
    "# Printing the output of model\n",
    "print(chat_model.invoke(prompt))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "314d893f-4b74-4e80-86bd-cf0a2a183c28",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "content='Covariance is a statistical measure that describes the relationship between two random variables. It indicates whether there is a positive or negative relationship between the variables, as well as the strength of that relationship. A positive covariance indicates that as one variable increases, the other variable also tends to increase, while a negative covariance indicates that as one variable increases, the other tends to decrease. A covariance of zero indicates that there is no relationship between the variables.' additional_kwargs={'refusal': None} response_metadata={'token_usage': {'completion_tokens': 88, 'prompt_tokens': 13, 'total_tokens': 101, 'completion_tokens_details': {'audio_tokens': None, 'reasoning_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': None, 'cached_tokens': 0}}, 'model_name': 'gpt-3.5-turbo-0125', 'system_fingerprint': None, 'finish_reason': 'stop', 'logprobs': None} id='run-9efb7763-fb79-4540-975e-045146a04de2-0' usage_metadata={'input_tokens': 13, 'output_tokens': 88, 'total_tokens': 101}\n"
     ]
    }
   ],
   "source": [
    "# Set the ChatOpenAI KEY and initialise a model\n",
    "chat_model = ChatOpenAI(api_key=OPEN_API_KEY)\n",
    "\n",
    "# creating a promp\n",
    "prompt = \"What is covarience?\"\n",
    "\n",
    "# Printing the output of model\n",
    "print(chat_model.invoke(prompt))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d3c7bbc-2362-4962-aff2-4875990a2166",
   "metadata": {},
   "source": [
    "# Prompts - can be a \"string\" or \"chat messages\"\n",
    "# Template - Logic1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "4a641382-d755-4fc7-888d-8336511ce7cb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['topic']"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "prompt_template = ChatPromptTemplate.from_messages([\n",
    "(\"system\",\"You are a helpful AI Tutor with expertise in Data Science and Artificial Intelligence.\"),\n",
    "    (\"human\", \"What is {topic}?\"),\n",
    "])\n",
    "prompt_template.input_variables"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba95dc20-c5b4-440a-988b-f4e7610b09df",
   "metadata": {},
   "source": [
    "# Chains\n",
    "\n",
    "### Chains allows us to link the output of one LLM call as the input of another call."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "3548588f-f3d6-454c-a870-34c93fe6dfea",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='NLP stands for Natural Language Processing. It is a field of artificial intelligence that focuses on the interaction between computers and humans using natural language. NLP allows computers to understand, interpret, and generate human language, enabling machines to communicate with humans in a more natural way. NLP techniques are used in various applications such as chatbots, sentiment analysis, language translation, and text summarization.', additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 78, 'prompt_tokens': 31, 'total_tokens': 109, 'completion_tokens_details': {'audio_tokens': None, 'reasoning_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': None, 'cached_tokens': 0}}, 'model_name': 'gpt-3.5-turbo-0125', 'system_fingerprint': None, 'finish_reason': 'stop', 'logprobs': None}, id='run-a6e05e86-3ae5-4f7f-ada5-3d4ba8d6b5a6-0', usage_metadata={'input_tokens': 31, 'output_tokens': 78, 'total_tokens': 109})"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chain = prompt_template | chat_model\n",
    "\n",
    "input = {'topic':\"NLP\"}\n",
    "\n",
    "chain.invoke(input)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e9b282a-2999-4f4d-8b2d-969cd7b76287",
   "metadata": {},
   "source": [
    "# Output Parsing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "2941a714-a760-4247-a76a-ae145f9ae225",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.output_parsers import StrOutputParser\n",
    "output_parser = StrOutputParser()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "1ada6538-88bb-4d34-ba39-cd146ab424fe",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'NLP stands for Natural Language Processing. It is a branch of artificial intelligence that focuses on the interaction between computers and humans using natural language. NLP enables computers to understand, interpret, and generate human language in a way that is valuable. This field involves tasks such as text/speech recognition, sentiment analysis, language translation, and more. NLP techniques are widely used in various applications such as chatbots, virtual assistants, language translation services, and text analysis tools.'"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Modifying the chain\n",
    "\n",
    "chain = prompt_template | chat_model | output_parser\n",
    "\n",
    "input = {'topic':\"NLP\"}\n",
    "\n",
    "chain.invoke(input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "19b17534-63b7-4683-8586-853cd951c6b5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Deep learning is a subset of machine learning that utilizes artificial neural networks to model and solve complex problems. These neural networks consist of multiple layers of interconnected nodes that process and learn from data to make predictions or decisions. Deep learning algorithms are capable of automatically learning hierarchical representations of data, extracting relevant features, and making accurate predictions or classifications. Deep learning has been successfully applied to various tasks such as image and speech recognition, natural language processing, and more.'"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Modifying the chain\n",
    "\n",
    "chain = prompt_template | chat_model | output_parser\n",
    "\n",
    "input = {'topic':\"Deep Learning\"}\n",
    "\n",
    "chain.invoke(input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c88e13ed-dcfa-48b3-962b-1a8020abcc60",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "669bd5af-39fd-4bc4-af88-d9cbf5d1f074",
   "metadata": {},
   "source": [
    "# Example : Create an AI Tutor APP that uses Prompts and Chat internally to given Python implementation tutorial for Data Science Topics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "6aafbb7e-1662-4ce2-afb9-f2ad64fcc1b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import OpenAI ChatModel\n",
    "from langchain_openai import ChatOpenAI\n",
    "# Set the ChatOpenAI KEY and initialise a model\n",
    "chat_model = ChatOpenAI(api_key=OPEN_API_KEY)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7529578-83a5-4976-b62c-8de46da24292",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "49b2120f-1761-4443-91f1-c8fb52bed3fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "f = open('keys/.gemini.txt')\n",
    "GOOGLE_API_KEY = f.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "d763904d-ea6a-4361-8e37-d518c076da8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_google_genai import ChatGoogleGenerativeAI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "b001f0d6-189e-41d5-81ef-d3c39ed12145",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "content='There are **28 states** and **8 union territories** in India. \\n' additional_kwargs={} response_metadata={'prompt_feedback': {'block_reason': 0, 'safety_ratings': []}, 'finish_reason': 'STOP', 'safety_ratings': [{'category': 'HARM_CATEGORY_SEXUALLY_EXPLICIT', 'probability': 'NEGLIGIBLE', 'blocked': False}, {'category': 'HARM_CATEGORY_HATE_SPEECH', 'probability': 'NEGLIGIBLE', 'blocked': False}, {'category': 'HARM_CATEGORY_HARASSMENT', 'probability': 'NEGLIGIBLE', 'blocked': False}, {'category': 'HARM_CATEGORY_DANGEROUS_CONTENT', 'probability': 'NEGLIGIBLE', 'blocked': False}]} id='run-a25306a1-bafc-4882-b637-3f8b89b9db6b-0' usage_metadata={'input_tokens': 9, 'output_tokens': 16, 'total_tokens': 25}\n"
     ]
    }
   ],
   "source": [
    "llm = ChatGoogleGenerativeAI(api_key=GOOGLE_API_KEY, model=\"gemini-1.5-flash\", temperature=0.9)\n",
    "prompt = \"How many states are there in India?\"\n",
    "print(llm.invoke(prompt))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "217cae8e-6532-4219-b0f7-1b007380823d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12409ed3-4a8e-4478-b43d-9e9cf1f25a09",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "725f6f69-b218-4fe7-bda2-19376ca850d1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ChatPromptTemplate(input_variables=['topic_name'], input_types={}, partial_variables={}, messages=[SystemMessagePromptTemplate(prompt=PromptTemplate(input_variables=[], input_types={}, partial_variables={}, template='You are a friendly AI Tutor with expertise in Data Science and AI who tells step by step Python implementation for topics asked by user.'), additional_kwargs={}), HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=['topic_name'], input_types={}, partial_variables={}, template='Tell me a python implementation for {topic_name}.'), additional_kwargs={})])"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate, SystemMessagePromptTemplate, HumanMessagePromptTemplate\n",
    "# construct system prompt\n",
    "system_prompt = SystemMessagePromptTemplate.from_template(\"\"\"You are a friendly AI Tutor with expertise in Data Science and AI who tells step by step Python implementation for topics asked by user.\"\"\")\n",
    "\n",
    "human_prompt = HumanMessagePromptTemplate.from_template(\"Tell me a python implementation for {topic_name}.\")\n",
    "\n",
    "chat_template = ChatPromptTemplate.from_messages([system_prompt, human_prompt])\n",
    "\n",
    "chat_template"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "433a0e33-acb6-47bc-af1a-1ebe58542e0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.output_parsers import StrOutputParser\n",
    "output_parser = StrOutputParser()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "d2f7390b-9f94-4a12-bb8f-1aac0f973c6b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sure! Here is a step-by-step Python implementation for Logistic Regression using the popular machine learning library `scikit-learn`:\n",
      "\n",
      "1. Import the necessary libraries:\n",
      "```python\n",
      "import numpy as np\n",
      "import pandas as pd\n",
      "from sklearn.model_selection import train_test_split\n",
      "from sklearn.linear_model import LogisticRegression\n",
      "from sklearn.metrics import accuracy_score, confusion_matrix\n",
      "```\n",
      "\n",
      "2. Load your dataset (for this example, I'll use a sample dataset from scikit-learn):\n",
      "```python\n",
      "from sklearn.datasets import load_iris\n",
      "data = load_iris()\n",
      "X = data.data\n",
      "y = data.target\n",
      "```\n",
      "\n",
      "3. Split the dataset into training and testing sets:\n",
      "```python\n",
      "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
      "```\n",
      "\n",
      "4. Initialize the Logistic Regression model and fit it to the training data:\n",
      "```python\n",
      "model = LogisticRegression()\n",
      "model.fit(X_train, y_train)\n",
      "```\n",
      "\n",
      "5. Make predictions on the test set:\n",
      "```python\n",
      "predictions = model.predict(X_test)\n",
      "```\n",
      "\n",
      "6. Evaluate the model by calculating accuracy and confusion matrix:\n",
      "```python\n",
      "accuracy = accuracy_score(y_test, predictions)\n",
      "conf_matrix = confusion_matrix(y_test, predictions)\n",
      "\n",
      "print(\"Accuracy: \", accuracy)\n",
      "print(\"Confusion Matrix: \")\n",
      "print(conf_matrix)\n",
      "```\n",
      "\n",
      "This is a basic implementation of Logistic Regression using scikit-learn in Python. Make sure to preprocess your data, handle missing values, and perform feature scaling if necessary before fitting the model. Let me know if you have any questions or need further clarification!\n"
     ]
    }
   ],
   "source": [
    "chain = chat_template | chat_model | output_parser\n",
    "\n",
    "input = {'topic_name':\"Logistic Regression\"}\n",
    "\n",
    "response = chain.invoke(input)\n",
    "\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f636d31-bcd2-422a-9868-dd1fb6f05da3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "7392a9e6-a667-4e34-8e5f-0f6ec9620746",
   "metadata": {},
   "source": [
    "# CSV Parser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "131de814-1ad7-46af-841c-057a7e4acc47",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Your response should be a list of comma separated values, eg: `foo, bar, baz` or `foo,bar,baz`'"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_core.output_parsers import CommaSeparatedListOutputParser\n",
    "csv_output_parser = CommaSeparatedListOutputParser()\n",
    "csv_output_parser.get_format_instructions()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "bc7eea29-5cf9-4311-83f8-658f75b1e34d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "str"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "example_input = \"Python, DA, SQL, ML, DL, AI\"\n",
    "type(example_input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "27074468-4589-4b41-9412-1ff1a8b46da5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Python', 'DA', 'SQL', 'ML', 'DL', 'AI']"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "csv_output_parser.parse(example_input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "baefabd5-116b-42de-bcdf-927f2f77566a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ChatPromptTemplate(input_variables=['dish_name', 'output_format_instructions'], input_types={}, partial_variables={}, messages=[SystemMessagePromptTemplate(prompt=PromptTemplate(input_variables=['output_format_instructions'], input_types={}, partial_variables={}, template=' You are a helpful AI chef Assistant. Given a dish name by user, you can provide the ingredients to prepare the dish. Output Format Instructions : {output_format_instructions} '), additional_kwargs={}), HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=['dish_name'], input_types={}, partial_variables={}, template='Give me the ingredient for cooking {dish_name}.'), additional_kwargs={})])"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate, SystemMessagePromptTemplate, HumanMessagePromptTemplate\n",
    "# construct system prompt\n",
    "system_prompt = SystemMessagePromptTemplate.from_template(\"\"\" You are a helpful AI chef Assistant. Given a dish name by user, you can provide the ingredients to prepare the dish. Output Format Instructions : {output_format_instructions} \"\"\")\n",
    "human_prompt = HumanMessagePromptTemplate.from_template(\"Give me the ingredient for cooking {dish_name}.\")\n",
    "\n",
    "chat_template = ChatPromptTemplate.from_messages([system_prompt, human_prompt])\n",
    "\n",
    "chat_template"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "abe20ffb-07b5-4e65-8bd2-90a48aba9712",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['paneer', 'basmati rice', 'onions', 'tomatoes', 'yogurt', 'ginger', 'garlic', 'green chilies', 'biryani masala', 'turmeric powder', 'red chili powder', 'garam masala', 'mint leaves', 'coriander leaves', 'lemon juice', 'ghee', 'oil', 'salt', 'water']\n"
     ]
    }
   ],
   "source": [
    "chain = chat_template | chat_model | csv_output_parser\n",
    "\n",
    "user_input = {'dish_name':\"paneer biryani\", \"output_format_instructions\" : csv_output_parser.get_format_instructions()}\n",
    "\n",
    "response = chain.invoke(user_input)\n",
    "\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7f6ff97-44c0-4b62-b64d-39eb405bfb69",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "9984025a-9a45-415b-bddc-7b78e8476331",
   "metadata": {},
   "source": [
    "# Datetime Parser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "1f1322dc-cff0-4612-ba19-5f37f66d60be",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: langchain in d:\\software\\lib\\site-packages (0.3.3)\n",
      "Requirement already satisfied: PyYAML>=5.3 in d:\\software\\lib\\site-packages (from langchain) (6.0.1)\n",
      "Requirement already satisfied: SQLAlchemy<3,>=1.4 in d:\\software\\lib\\site-packages (from langchain) (2.0.30)\n",
      "Requirement already satisfied: aiohttp<4.0.0,>=3.8.3 in d:\\software\\lib\\site-packages (from langchain) (3.9.5)\n",
      "Requirement already satisfied: langchain-core<0.4.0,>=0.3.10 in d:\\software\\lib\\site-packages (from langchain) (0.3.10)\n",
      "Requirement already satisfied: langchain-text-splitters<0.4.0,>=0.3.0 in d:\\software\\lib\\site-packages (from langchain) (0.3.0)\n",
      "Requirement already satisfied: langsmith<0.2.0,>=0.1.17 in d:\\software\\lib\\site-packages (from langchain) (0.1.125)\n",
      "Requirement already satisfied: numpy<2.0.0,>=1.26.0 in d:\\software\\lib\\site-packages (from langchain) (1.26.4)\n",
      "Requirement already satisfied: pydantic<3.0.0,>=2.7.4 in d:\\software\\lib\\site-packages (from langchain) (2.9.2)\n",
      "Requirement already satisfied: requests<3,>=2 in d:\\software\\lib\\site-packages (from langchain) (2.32.2)\n",
      "Requirement already satisfied: tenacity!=8.4.0,<9.0.0,>=8.1.0 in d:\\software\\lib\\site-packages (from langchain) (8.2.2)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in d:\\software\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.2.0)\n",
      "Requirement already satisfied: attrs>=17.3.0 in d:\\software\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (24.2.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in d:\\software\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.4.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in d:\\software\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (6.0.4)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in d:\\software\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.9.3)\n",
      "Requirement already satisfied: jsonpatch<2.0,>=1.33 in d:\\software\\lib\\site-packages (from langchain-core<0.4.0,>=0.3.10->langchain) (1.33)\n",
      "Requirement already satisfied: packaging<25,>=23.2 in d:\\software\\lib\\site-packages (from langchain-core<0.4.0,>=0.3.10->langchain) (23.2)\n",
      "Requirement already satisfied: typing-extensions>=4.7 in d:\\software\\lib\\site-packages (from langchain-core<0.4.0,>=0.3.10->langchain) (4.11.0)\n",
      "Requirement already satisfied: httpx<1,>=0.23.0 in d:\\software\\lib\\site-packages (from langsmith<0.2.0,>=0.1.17->langchain) (0.26.0)\n",
      "Requirement already satisfied: orjson<4.0.0,>=3.9.14 in d:\\software\\lib\\site-packages (from langsmith<0.2.0,>=0.1.17->langchain) (3.10.6)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in d:\\software\\lib\\site-packages (from pydantic<3.0.0,>=2.7.4->langchain) (0.6.0)\n",
      "Requirement already satisfied: pydantic-core==2.23.4 in d:\\software\\lib\\site-packages (from pydantic<3.0.0,>=2.7.4->langchain) (2.23.4)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in d:\\software\\lib\\site-packages (from requests<3,>=2->langchain) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in d:\\software\\lib\\site-packages (from requests<3,>=2->langchain) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in d:\\software\\lib\\site-packages (from requests<3,>=2->langchain) (2.2.2)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in d:\\software\\lib\\site-packages (from requests<3,>=2->langchain) (2024.7.4)\n",
      "Requirement already satisfied: greenlet!=0.4.17 in d:\\software\\lib\\site-packages (from SQLAlchemy<3,>=1.4->langchain) (3.0.1)\n",
      "Requirement already satisfied: anyio in d:\\software\\lib\\site-packages (from httpx<1,>=0.23.0->langsmith<0.2.0,>=0.1.17->langchain) (4.2.0)\n",
      "Requirement already satisfied: httpcore==1.* in d:\\software\\lib\\site-packages (from httpx<1,>=0.23.0->langsmith<0.2.0,>=0.1.17->langchain) (1.0.2)\n",
      "Requirement already satisfied: sniffio in d:\\software\\lib\\site-packages (from httpx<1,>=0.23.0->langsmith<0.2.0,>=0.1.17->langchain) (1.3.0)\n",
      "Requirement already satisfied: h11<0.15,>=0.13 in d:\\software\\lib\\site-packages (from httpcore==1.*->httpx<1,>=0.23.0->langsmith<0.2.0,>=0.1.17->langchain) (0.14.0)\n",
      "Requirement already satisfied: jsonpointer>=1.9 in d:\\software\\lib\\site-packages (from jsonpatch<2.0,>=1.33->langchain-core<0.4.0,>=0.3.10->langchain) (2.1)\n"
     ]
    }
   ],
   "source": [
    "!pip install langchain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "70eb6e58-41cb-4381-a658-fc4d81c2e84a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.output_parsers import DatetimeOutputParser\n",
    "output_parser = DatetimeOutputParser()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "ff7a05aa-70f0-4173-8cb0-c0ee530e379e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Write a datetime string that matches the following pattern: '%Y-%m-%dT%H:%M:%S.%fZ'.\\n\\nExamples: 0029-11-03T02:45:34.777941Z, 0081-02-17T18:35:55.348793Z, 0381-09-14T02:16:18.353714Z\\n\\nReturn ONLY this string, no other words!\""
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output_parser.get_format_instructions()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "d58ede58-bd2b-498b-8b23-cf99a9bce549",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.prompts import PromptTemplate\n",
    "\n",
    "template = \"\"\"Answer the users question:\n",
    "Question : {question}\n",
    "Output Format Instructions : {output_format_instructions} \"\"\"\n",
    "prompt_template = PromptTemplate.from_template(template)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "0f067226-5cae-41fd-8637-38a7ff8d315f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1947-08-15 00:00:00\n"
     ]
    }
   ],
   "source": [
    "chain = prompt_template | chat_model | output_parser\n",
    "\n",
    "input = {\"question\": \"What is Indian Independence Day?\", \n",
    "         \"output_format_instructions\" : output_parser.get_format_instructions()}\n",
    "\n",
    "response = chain.invoke(input)\n",
    "\n",
    "print(response)\n",
    "         "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "374e876b-1ecc-42f5-a51b-96a27489d081",
   "metadata": {},
   "source": [
    "# Pydantic Parser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de8974d6-0d14-493d-bbcf-7a5f3cab3678",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
